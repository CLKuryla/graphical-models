---
title: "P8124 Assignment 3"
author: "Christine Lucille Kuryla (clk2162)"
date: "2023-10-23"
output:
  pdf_document:
    toc: no
  html_document:
    toc: yes
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(broom)
```

#     Problem 2

## Simulate data from given MRF independence model

```{r problem_2a}

library(MASS)

# simulate data from a given MRF independence model

set.seed(123)
( K <- cbind(c(10,7,7,0),c(7,20,0,7),c(7,0,30,7),c(0,7,7,40)) )
data <- as.data.frame(mvrnorm(n=10000,mu=c(0,0,0,0),Sigma=solve(K)))
colnames(data) <- c("X1","X2","X3","X4")

# (Note: in R, the inverse of a matrix M is obtained by solve(M).) 

```


### Conditional Independencies

*What are the conditional independencies that are representing in this precision matrix?*
Conditional independencies correspond to the zeros in the precision matrix of the elements given everything else. Hence, for K, the conditional independencies are:

$X_1 \perp X_4 | X_2, X_3$ 

and 

$X_2 \perp X_3 | X_1, X_4$

### Corresponding Graph (INSERT PIC?!?!?!?!)
*What is the corresponding graph?*
The corresponding MRF has vertices $X_1, X_2, X_3, X_4$ and edges:  

 - $X_1 - X_2$. 
 
 - $X_2 - X_4$. 
 
 - $X_4 - X_3$. 
 
 - $X_3 - X_1$.

### Verify with linear regression
*Verify the conditional independence constraints by using linear regression.*

```{r q2_regression}

# X1 independent of X4 given X2, X3
summary(glm(data = data, formula = X1 ~ X4 + X2 + X3))

# X2 independent of X3 given X1, X4
summary(glm(data = data, formula = X2 ~ X3 + X1 + X4))

```

As demonstrated in the first linear regression, $X_1 \perp X_4 | X_2, X_3$ because we can see that when regressing $X_1$ on $X_4, X_2,X_3$ gives a large p-value for $X_4$ because they are conditionally independent since $X_2$ and $X_3$ are given (note that their small p values demonstrate that they are dependent). The same logic applies to the second regression for $X_2 \perp X_3 | X_1, X_4$ by regressing $X_2$ on the rest of the variables and observing a large p-value for $X_3$, showing independence, because $X_1$ and $X_4$ are conditioned on by putting them in the regression.  

### Explanation

The zeroes in the precision matrix K correspond to the conditional independencies described above. The MRF is the UG with $X_1, X_2, X_3, X_4$ that has the edge between $X_1$ and $X_4$ removed because of the conditional independence $X_1 \perp X_4 | X_2, X_3$ and the edge between $X_2$ and $X_3$ removed because of the conditional independence $X_2 \perp X_3 | X_1, X_4$ that were demonstrated by the zeroes in the precision matrix. The linear regression demonstrates that the conditional independencies are true because when one variable is regressed on the rest, the p-value for the variable that it is conditionally independent of is large (because they are independent), and the p-values of the variables in the conditioning set are small (because they are dependent). 


## Estimate precision matrix subject to graph constraints
```{r problem_2b}

# Use the gRim package to fit the model, i.e., estimate the precision matrix subject to the graph constraints.

library(gRim)
glist <- list( c("X1","X2"), c("X2","X4"), c("X4","X3"), c("X3","X1") )
ddd <- cov.wt(data, method="ML")
fit <- ggmfit(ddd$cov, ddd$n.obs, glist) # Estimate parameters using IPF
fit$K # estimated precision matrix

# Did it work? How do you know?

# Precision matrix (K)
kable(K)

# Estimated precision matrix 
kable(fit$K)

```

Yes, it worked. We know this because the estimated precision matrix has the expected zeroes that correspond to the conditional independencies, and in general, the values are quite close to K so a good estimation of the actual precision matrix. 

# Problem 3

Consider the Gaussian Bayesian Network model with the following covariance matrix:
and the DAG G with edges X1 → X2 ← X3 and X4 → X2.

## a) Correlation constraints and correlation matrix
* a) What correlation constraints does this model represent? Estimate the correlation matrix. *
This model represents three marginal independencies (six correlations shown by the 0s). 

- $X_4 \perp X_3$ ($X_4$ is marginally independent of $X_3$, so the correlation between $X_4$ and $X_3$ = 0). Correlations are symmetric, so $corr(X_3, X_4) = corr(X_4, X_3) = 0$.

- $X_1 \perp X_3$ ($X_1$ is marginally independent of $X_3$, so the correlation between $X_1$ and $X_3$ = 0). Correlations are symmetric, so $corr(X_3, X_1) = corr(X_1, X_3) = 0$.

- $X_1 \perp X_4$ ($X_1$ is marginally independent of $X_4$, so the correlation between $X_1$ and $X_4$ = 0). Correlations are symmetric, so $corr(X_4, X_1) = corr(X_1, X_4) = 0$.

```{r problem_3_corr_matrix}

set.seed(123)
( Sig <- cbind(c(3,-1.4,0,0),c(-1.4,3,1.4,1.4),c(0,1.4,3,0),c(0,1.4,0,3)) )
data <- as.data.frame(mvrnorm(n=10000,mu=c(0,0,0,0),Sigma=Sig))
colnames(data) <- c("X1","X2","X3","X4")

# Estimate correlation matrix
sigma_est <- cor(data)

kable(sigma_est)

```


## b) The moralized graph
* b) Consider also the moralized graph Gm and what the corresponding precision matrix K would look like. What are the partial correlation constraints represented in K? How does this make sense with respect to sigma above? *

- The moralized Graph Gm would be the complete graph formed from the skeleton of G. It is the graph formed by making the edges in G undirected and adding edges $X_1 - X_4$, $X_4 - X_3$, and $X_3 - X_1$ because $X_2$ is an unshielded collider so it's parents are married during the moralization process. 
- There are no partial correlation constraints represented in K because there are no missing edges in Gm. XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
- This makes sense wrt the correlation matrix Sigma above because there are marginal independencies but no conditional independencies. XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

## c) Estimate K, take inverse, and compare to true Sigma
* c) Following steps similar to the previous problem, estimate the corresponding precision matrix K from this data (using ggmfit). Take the inverse and compare to the true covariance matrix. *

```{r problem_3_c}

glist <- list( c("X1","X2"), c("X2","X3"), c("X4","X2")  )
ddd <- cov.wt(data, method="ML")
fit <- ggmfit(ddd$cov, ddd$n.obs, glist) # Estimate parameters using IPF
fit$K # estimated precision matrix

solve(fit$K) # inverse of K (covariance matrix)

# True covariance matrix
Sig

```

The estimates for the non-zero entries in the covariance matrix are similar numbers to the true covariance matrix (close to -1.4, 1.4, -3, 3, but not exact because it's an estimation from simulated data). However, there are very non-zero values in place of the true zeroes, because during the moralization process, three edges were added because of the unshielded colliders. This is a demonstration of why going from a moralized, undirected graph Gm to a DAG is not a reliable way to completely determine the structure of the original DAG G. 

# Problem 4

## Simulate graph
```{r problem_4}
library(dagitty)

#Use dagitty to simulate 10000 observations from this graph:
g <- dagitty( "dag{ x <- u1; u1 -> m <- u2 ; u2 -> y }" )

sim_sem <- simulateSEM(g,
            b.lower = 0.4,
            b.upper = 0.7,
            N = 10000)

# Here U1,U2 represent unmeasured variables.
```

## Estimate effects of X on Y

*Estimate the effect of X on Y adjusting for M in a linear regression, obtaining a 95% confidence interval for the effect.*
```{r problem_4b}
# Estimate the effect of X on Y adjusting for M in a linear regression, obtaining a 95% confidence interval for the effect. 

lm_m = lm(data = sim_sem, formula = y ~ x + m )
summary(lm_m)
library(broom)
tidy_ci_m <- tidy(lm_m, conf.int=TRUE)
tidy_ci_m
```

The estimated effect of X on Y, adjusting for M, is `r tidy_ci_m$estimate[2]`, with a 95% confidence interval of (tidy_ci_m$conf.low[2], tidy_ci_m$conf.high[2]). 

* Then estimate the same effect (and confidence interval) using the correct sufficient adjustment set that you can obtain from dagitty.*
```{r problem_4c}
# Then estimate the same effect (and confidence interval) using the correct sufficient adjustment set that you can obtain from dagitty. 

# Sufficient adjustment set
adjustmentSets(g, "y", "x", type = "minimal")
# This results in an empty set. 

lm_sufficient = lm(data = sim_sem, formula = y ~ x )
summary(lm_sufficient)
tidy_ci_sufficient <- tidy(lm_sufficient, conf.int=TRUE)
tidy_ci_sufficient

```

The estimate of the effect of X on Y adjusting for M in a linear regression is `r tidy_ci_m$estimate[2]`, with a confidence interval (`r tidy_ci_m$conf.low[2]`, `r tidy_ci_m$conf.high[2]`). This does not cross zero and implies an association. 

The estimate of the effect of X on Y adjusting for nothing (the sufficient set from daggity was the empty set) in a linear regression is `r tidy_ci_sufficient$estimate[2]`, with a confidence interval (`r tidy_ci_sufficient$conf.low[2]`, `r tidy_ci_sufficient$conf.high[2]`). This crosses zero and does not imply an association. 

From this example, we can conclude that adjusting for M induced an association that was not actually there. This is an example of the M bias, or the butterfly bias, where a d-connection is induced by conditioning on the wrong thing, so it results in a bias/association that isn't actually there. We can see this from the underlying data that we simulated from a known DAG. 

# Problem 5

## Construct DAG in dagitty and simulate 10000 observations

```{r problem_5}
# Construct the DAG in Figure 1 as a daggity object. 

dag_q5 <- dagitty('dag {
    D [pos="0,1"]
    E [pos="1,1"]
    A [pos="2,1"]
    B [pos="3,1"]
    F [pos="4,1"]
    C [pos="1,0"]
    G [pos="4,0"]
    H [pos="5,0"]

    D -> E -> A <- B <- G -> H
    E -> F -> H
    E <- C -> H
    C -> B
    C -> F -> G

}')

plot(dag_q5)

# Simulate 10000 observations from this graph using simulateSEM() as you did on the first homework.

sim_sem <- simulateSEM(dag_q5,
            b.lower = -0.7,
            b.upper = 0.7,
            N = 100000)
```

## Estimate the effect of E on F with linear regression and different adjustments

Note that the result from dagitty for the minimal adjustment set is just one: { C }. In order to estimate the effect of E on F, we will regress F on E and adjust for C, as well as regressing it on everything, and compare the results. 

```{r problem_5a}
# Estimate the effect of E on F and the effect of B on A using backdoor adjustment and linear regression.

# Effect of E on F 

adjustmentSets(dag_q5, "E", "F", type = "minimal")
# Result: { C }

# Linear regression of F on E, adjusting for C
lm_ef_adj_c = lm(data = sim_sem, formula = F ~ E + C )
summary(lm_ef_adj_c)
tidy_ef_adj_c <- tidy(lm_ef_adj_c, conf.int=TRUE)
tidy_ef_adj_c

# Linear regression of F on everything
lm_f = lm(data = sim_sem, formula = F ~ E + C + A + B + D + G + H)
summary(lm_f)
tidy_f <- tidy(lm_f, conf.int=TRUE)
tidy_f

# Linear regression of F on E, with no adjustments
lm_ef_adj_0 = lm(data = sim_sem, formula = F ~ E)
summary(lm_ef_adj_0)
tidy_ef_adj_0 <- tidy(lm_ef_adj_0, conf.int=TRUE)
tidy_ef_adj_0

# Table of results

ef_adj_c <- c (tidy_ef_adj_c$term[2],
             tidy_ef_adj_c$estimate[2],
             tidy_ef_adj_c$std.error[2],
             tidy_ef_adj_c$conf.low[2],
             tidy_ef_adj_c$conf.high[2],
             (tidy_ef_adj_c$conf.high[2] - tidy_ef_adj_c$conf.low[2])
                )
ef_adj_all <- c (tidy_f$term[2],
             tidy_f$estimate[2],
             tidy_f$std.error[2],
             tidy_f$conf.low[2],
             tidy_f$conf.high[2],
             (tidy_f$conf.high[2] - tidy_f$conf.low[2])
                )

ef_adj_0 <- c (tidy_ef_adj_0$term[2],
             tidy_ef_adj_0$estimate[2],
             tidy_ef_adj_0$std.error[2],
             tidy_ef_adj_0$conf.low[2],
             tidy_ef_adj_0$conf.high[2],
             (tidy_ef_adj_0$conf.high[2] - tidy_ef_adj_0$conf.low[2])
                )

table_ef <- rbind(ef_adj_c, ef_adj_all, ef_adj_0)

colnames(table_ef) <- c("Var", "Estimate", "SE", "CI low", "CI high", "CI length")

kable(table_ef)

```


The estimate of the effect of E on F adjusting for C in a linear regression is `r tidy_ef_adj_c$estimate[2]`, with a confidence interval (`r tidy_ef_adj_c$conf.low[2]`, `r tidy_ef_adj_c$conf.high[2]`). The standard error is `r tidy_ef_adj_c$std.error[2]`. The CI length is `r tidy_ef_adj_c$conf.high[2] - tidy_ef_adj_c$conf.low[2]`.

The estimate of the effect of E on F adjusting for all of the variables in a linear regression is `r tidy_f$estimate[2]`, with a confidence interval (`r tidy_f$conf.low[2]`, `r tidy_f$conf.high[2]`). The standard error is `r tidy_f$std.error[2]`. The CI length is `r tidy_f$conf.high[2] - tidy_f$conf.low[2]`.

The estimate of the effect of E on F adjusting for none of the variables in a linear regression is `r tidy_f$estimate[2]`, with a confidence interval (`r tidy_f$conf.low[2]`, `r tidy_f$conf.high[2]`). The standard error is `r tidy_f$std.error[2]`. The CI length is `r tidy_f$conf.high[2] - tidy_f$conf.low[2]`.

## Estimate the effect of B on A with linear regression and different adjustments

Note that the result from dagitty for the minimal adjustment sets are { E }, { C, F }, { C, G }. In order to estimate the effect of B on A, we will regress A on B and adjust for the sufficient sets, as well as regressing it on everything, and compare the results. 

```{r problem_5b}

# Effect of B on A 
adjustmentSets(dag_q5, "B", "A", type = "minimal")
# Result: { E }, { C, F }, { C, G }

# If there is more than one sufficient adjustment set, try each of the ones identified by dagitty and compare them. 


# Linear regression of A on B, adjusting for E
lm_ab_adj_e = lm(data = sim_sem, formula = A ~ B + E )
summary(lm_ab_adj_e)
tidy_ab_adj_e <- tidy(lm_ab_adj_e, conf.int=TRUE)
tidy_ab_adj_e

# Linear regression of A on B, adjusting for C and F
lm_ab_adj_cf = lm(data = sim_sem, formula = A ~ B + C + F )
summary(lm_ab_adj_cf)
tidy_ab_adj_cf <- tidy(lm_ab_adj_cf, conf.int=TRUE)
tidy_ab_adj_cf

# Linear regression of A on B, adjusting for C and G
lm_ab_adj_cg = lm(data = sim_sem, formula = A ~ B + C + G )
summary(lm_ab_adj_cg)
tidy_ab_adj_cg <- tidy(lm_ab_adj_cg, conf.int=TRUE)
tidy_ab_adj_cg

# Linear regression of A on B, adjusting for all variables
lm_ab_adj_all = lm(data = sim_sem, formula = A ~ B + C + D + E + F + G + H )
summary(lm_ab_adj_all)
tidy_ab_adj_all <- tidy(lm_ab_adj_all, conf.int=TRUE)
tidy_ab_adj_all

# Linear regression of A on B, adjusting for nothing
lm_ab_adj_0 = lm(data = sim_sem, formula = A ~ B )
summary(lm_ab_adj_0)
tidy_ab_adj_0 <- tidy(lm_ab_adj_0, conf.int=TRUE)
tidy_ab_adj_0

# Table of results

ab_adj_e <- c (tidy_ab_adj_e$term[2],
             tidy_ab_adj_e$estimate[2],
             tidy_ab_adj_e$std.error[2],
             tidy_ab_adj_e$conf.low[2],
             tidy_ab_adj_e$conf.high[2],
             (tidy_ab_adj_e$conf.high[2] - tidy_ab_adj_e$conf.low[2])
                )
ab_adj_cf <- c (tidy_ab_adj_cf$term[2],
             tidy_ab_adj_cf$estimate[2],
             tidy_ab_adj_cf$std.error[2],
             tidy_ab_adj_cf$conf.low[2],
             tidy_ab_adj_cf$conf.high[2],
             (tidy_ab_adj_cf$conf.high[2] - tidy_ab_adj_cf$conf.low[2])
                )

ab_adj_cg <- c (tidy_ab_adj_cg$term[2],
             tidy_ab_adj_cg$estimate[2],
             tidy_ab_adj_cg$std.error[2],
             tidy_ab_adj_cg$conf.low[2],
             tidy_ab_adj_cg$conf.high[2],
             (tidy_ab_adj_cg$conf.high[2] - tidy_ab_adj_cg$conf.low[2])
                )

ab_adj_all <- c (tidy_ab_adj_all$term[2],
             tidy_ab_adj_all$estimate[2],
             tidy_ab_adj_all$std.error[2],
             tidy_ab_adj_all$conf.low[2],
             tidy_ab_adj_all$conf.high[2],
             (tidy_ab_adj_all$conf.high[2] - tidy_ab_adj_all$conf.low[2])
                )

ab_adj_0 <- c (tidy_ab_adj_0$term[2],
             tidy_ab_adj_0$estimate[2],
             tidy_ab_adj_0$std.error[2],
             tidy_ab_adj_0$conf.low[2],
             tidy_ab_adj_0$conf.high[2],
             (tidy_ab_adj_0$conf.high[2] - tidy_ab_adj_0$conf.low[2])
                )

table_ab <- rbind(ab_adj_e, ab_adj_cf, ab_adj_cg, ab_adj_all, ab_adj_0)

colnames(table_ab) <- c("Var", "Estimate", "SE", "CI low", "CI high", "CI length")

kable(table_ab)


# Are the point estimates similar? 
# Do the estimates have similar variance (or confidence interval length)? 
# Compare also these estimates against an approach which simply adjusts for all other variables in the graph. 
# How are the results different (if they are) and what is the explanation?

```



`r kable(table_ef[,c(2,3,4,5)])`
`r kable(table_ab[,c(2,3,4,5)])`

Summary:

`r kable(table_ef[,c(2,3,6)])`
When exploring the effect of E on F, there was only one set of sufficient adjustment variables from dagitty. Compared to the model with all of the variables in the equation, the point estimates are not similar and the confidence intervals do not usually overlap. The point estimates also have relatively different variance (and CI length). Compared to the unadjusted model, the point estimates are different but the unadjusted variances/CI lengths are lower. 

`r kable(table_ab[,c(2,3,6)])`
For the effect of B on A, the point estimates are very similar for all three minimally sufficient sets, as are the variances (or confidence interval lengths). Additionally, they are all similar to the regression where all of the variables were adjusted for in the model. It seems that the sufficient adjustment sets do indeed sufficiently adjust as needed. Compared to the unadjusted model, the point estimates are different when no adjustment is performed, but the variances/CI lengths are similar. 

When comparing the results of the different adjustments for the effect of E on F versus the effect of B on A, we see that adjusting for all of the variables in the E on F model causes the effect estimate to change much more than for the model looking at the effect of B on A, and the variances/CI lengths are markedly different for E on F as well as compared to those of the B on A. Both models have different point estimates when adjusted. 